# The reagent project - Gentoo containers from scratch

buildah (OCI)
--------------

## Philosophy

The goal of this project was to create an end-to-end build pipeline that has few or no proprietary or "big shop" dependencies. It's based on gentoo and the libpod/buildah toolchain which allow you to build containers within a container thereby keeping all build dependencies isolated. I also self-imposed the requirement of having all components capable of being self-hosted, so no required tie-ins to public cloud providers. 

Perusing the codebase, the largest contribution of this project to date is the flexible imageList class located in pybuild.py. It allows you to set, track, and update states for containers based on image classification. For example, if you'd like to create a complex series of image dependencies with dockerfiles, add 20 new build stages whereby you run it through a battery of tests, simply import the pybuild.py module and override the appropriate class variables and you're in business.

Ultimately, the states and operations tracked by default are: Building the image, testing the image, running a vulnerability scanner (currently unimplemented), and pushing the image to a registry. 

I will eventually build a helm chart so that all of the components are parameterized and spun up on a k8s cluster, and images can be discovered and tracked inside jenkins. 

###### Project dependencies: 
* python3
* [buildah](https://github.com/projectatomic/buildah)
* private docker registry (Not required, but presumably you'd want some sort of artifact repo)
* [Gentoo Binhost](https://wiki.gentoo.org/wiki/Binary_package_guide) (not required, but strongly recommended)
* tls certs (optional, but assumed [https://])

## Build stages

Reagent acts as wrapper for several utilties that allow you to incorporate a full end-to-end build into a container.

usage: build [-h] [-v] [-p] [-c] [-i] [-b BUILD_TARGETS [BUILD_TARGETS ...]]
             [-t] [-T] [-R]

optional arguments:
  -h, --help            show this help message and exit
  -v, --verbose         Show verbose output from subprocess commands.
  -p, --portage         Regenerate a container with synced portage contents.
  -c, --catalyst        Build contents of .stages/default/ with catalyst using
                        specfiles.
  -i, --initial         Build all numbered buildah files in the root directory
                        in order with buildah.
  -b BUILD_TARGETS [BUILD_TARGETS ...], --build BUILD_TARGETS [BUILD_TARGETS ...]
                        Build selected contents matched by regex. Use 'all' to
                        build all leaf containers.
  -t, --test            Test images using OCIv1.config.Labels instruction
                        before pushing them to registry.
  -T, --vulnerability   Run vulnerability tests against images.
  -R, --disable-registry
                        Disable push to registry and cleanup.

## Design

By leveraging a binhost in conjunction with portage, the only rebuilds that become necessary are those that are necessary for upgrades or changes in useflags. This gives you full control at every step of the build while cutting down on buildtimes dramatically. Additionally, buildah does not snapshot your container storage after every build step. Snapshotting currently only occurs upon commit. If smaller images are necessary, it's strongly recommended to add a script to remove build dependencies. 

There are 3 models which are frequently used with image generation:
Size optimized (default)
Layer optimized
Build time optimized

The images generated by this utility are currently built for reduced size. This is done by installing the build deps (`--with-bdeps=y`) for the initial stage of each dockerfile, and running (`--with-bdeps=n`) prior to commit. As these are binary packages, this takes very little (but not negligable) time.

By altering the dockerfiles, it's easy to create a larger but universally common layer with all build tools on it and sharing that amongst all your container servers. This increases pressure on the network and storage infrastructure in propigating this image as part of your build pipeline, however it can be very useful if you have many leaf images and may reduce your overall network and storage consumption.

A third option exists which focuses on build time and is the antithesis of the layered approach. You may enable layer caching in buildah, however this may not accurately detect changes made to your dockerfile, nor to the underlying portage repository. This strategy focuses on building one master image and then removing utilities and creating `--squash`'ed leaf image. The resulting leaf images are monolithic and unique, however this will cut down on your build times in some circumstances.

###### update buildah:
```
export GOPATH="~/.go/"
cd ~/.go/src/github.com/projectatomic/buildah/
make -j$THREADS
sudo make install
```

###### Certificates: 
`git submodule add -b <branch> <url> .x509`
